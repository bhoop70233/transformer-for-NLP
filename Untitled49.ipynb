{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pbf6qF_94tc6"
      },
      "outputs": [],
      "source": [
        "!pip install datasets # install the datasets module\n",
        "from huggingface_hub import list_datasets\n",
        "\n",
        "# get a list of all datasets on the Hub\n",
        "all_datasets = list(list_datasets()) # convert the generator object to a list\n",
        "\n",
        "print(f\"there are {len(all_datasets)} datasets currently available on the Hub\")\n",
        "print(f\"The first 10 are: {all_datasets[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --upgrade\n",
        "from datasets import load_dataset # import load_dataset from the datasets module\n",
        "emotions = load_dataset(\"emotion\")"
      ],
      "metadata": {
        "id": "-zAsThxa5Y-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = {\n",
        "    'train': {'features': ['text', 'label'], 'num_rows': 16000},\n",
        "    'validation': {'features': ['text', 'label'], 'num_rows': 2000},\n",
        "    'test': {'features': ['text', 'label'], 'num_rows': 2000}\n",
        "}"
      ],
      "metadata": {
        "id": "VqEmNGB27v2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = emotions[\"train\"]\n",
        "train_ds\n"
      ],
      "metadata": {
        "id": "L10ftJ-H8H2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_ds)"
      ],
      "metadata": {
        "id": "FZPbEpgd8wUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds['features'] # Access the value associated with the key 'features'\n"
      ],
      "metadata": {
        "id": "1v1gHhYV857d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "emotions = load_dataset(\"emotion\")\n",
        "\n",
        "# Print the DatasetDict\n",
        "print(emotions)\n",
        "\n",
        "# Access and print the 'train' dataset\n",
        "train_ds = emotions['train']\n",
        "print(train_ds)\n",
        "\n",
        "# Print the features of the 'train' dataset\n",
        "print(train_ds.features)\n",
        "\n",
        "# Print the first example from the 'train' dataset\n",
        "print(train_ds[0])"
      ],
      "metadata": {
        "id": "Hu06f9WI9B8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(train_ds.features)"
      ],
      "metadata": {
        "id": "TX_WlVU-9NIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_ds[:5])"
      ],
      "metadata": {
        "id": "nVwrTul39ujt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_ds[\"text\"][:5])"
      ],
      "metadata": {
        "id": "auhxxyKe95SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "emotions.set_format(type=\"pandas\")\n",
        "df=emotions[\"train\"][:]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "XupSTNXI-F-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_int2str(row):\n",
        "  return emotions[\"train\"].features[\"label\"].int2str(row)\n",
        "df[\"label_name\"]=df[\"label\"].apply(label_int2str)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KP1amZmj_RhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
        "plt.title(\"Frequency of Classes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s-Z2NLwZAFcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Words per Tweet\"]=df[\"text\"].str.split().apply(len)\n",
        "df.boxplot(\"Words per Tweet\",by=\"label_name\",grid=False,showfliers=False,color=\"black\") # Changed \"words per tweet\" to \"Words per Tweet\"\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fXtBQbXGIldV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions.reset_format()"
      ],
      "metadata": {
        "id": "xql3yuA5Joaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Tokenizing text is a core task of NLP.\"\n",
        "tokenized_text=list(text)\n",
        "print(tokenized_text)\n"
      ],
      "metadata": {
        "id": "x2WbUuk5Ka4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token2idx={ch:idx for idx ,ch in enumerate(sorted(set(tokenized_text)))}\n",
        "print(token2idx)"
      ],
      "metadata": {
        "id": "EhR4TLHNK9V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids=[token2idx[token] for token in tokenized_text]\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "AnOGMI_aLXmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_df=pd.DataFrame(\n",
        "    {\"Name\":[\"Bumblebee\",\"Optimus prime\",\"Megatron\"],\"Label ID\":[0,1,2]})\n",
        "categorical_df"
      ],
      "metadata": {
        "id": "WOlph0omL0G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(categorical_df[\"Name\"])"
      ],
      "metadata": {
        "id": "u09ISCwhMele"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "input_ids=torch.tensor(input_ids)\n",
        "one_hot_encodings=F.one_hot(input_ids,num_classes=len(token2idx))\n",
        "one_hot_encodings.shape\n"
      ],
      "metadata": {
        "id": "UsPduiuNMvFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Token:{tokenized_text[0]}')\n",
        "print(f\"Tensor index:{input_ids[0]}\")\n",
        "print(f\"One-hot:{one_hot_encodings[0]}\")"
      ],
      "metadata": {
        "id": "JyNkMEQWNg6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text=text.split()\n",
        "print(tokenized_text)\n"
      ],
      "metadata": {
        "id": "62E2ioBHOG5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "model_ckpt=\"distilbert-base-uncased\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "4V2ptQYCOu_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "distilbert_tokenizer=DistilBertTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "sXxHBun1QGp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text=tokenizer(text)\n",
        "print(encoded_text)"
      ],
      "metadata": {
        "id": "DHgOjdU2Qhar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "61xGXpcKQ5bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_tokens_to_string(tokens))"
      ],
      "metadata": {
        "id": "xVfjAs9GRRCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "p79c2fwERqze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "id": "LN1ywEafR0Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_input_names"
      ],
      "metadata": {
        "id": "s_1QUyjzR8nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "  return tokenizer(batch[\"text\"],padding=True,truncation=True)"
      ],
      "metadata": {
        "id": "0v_h8UzYSE9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenize(emotions[\"train\"][:2]))"
      ],
      "metadata": {
        "id": "dusc0D6RSvXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions_encoded=emotions.map(tokenize,batched=True,batch_size=None)"
      ],
      "metadata": {
        "id": "0PNrjt_FS_q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(emotions_encoded[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "Jgau5UTETk8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers  import AutoModel\n",
        "model_ckpt=\"distilbert-base-uncased\"\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=AutoModel.from_pretrained(model_ckpt).to(device)"
      ],
      "metadata": {
        "id": "98bMZ4hdT2Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"this is a text\"\n",
        "inputs=tokenizer(text,return_tensors=\"pt\")\n",
        "print(f\"Input tensor shape:{inputs['input_ids'].size()}\")\n"
      ],
      "metadata": {
        "id": "iDxtaZdXVbSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs={k:v.to(device) for k,v in inputs.items()}\n",
        "with torch.no_grad():\n",
        "  outputs=model(**inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "lTRER6VUWwS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.last_hidden_state.size()\n",
        "torch.Size([1,6,768])"
      ],
      "metadata": {
        "id": "ijcWQZDWXHkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.last_hidden_state[:,0].size()\n"
      ],
      "metadata": {
        "id": "jKJWWgfiXiTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hidden_states(batch):\n",
        "  #Place model inputs on the GPU\n",
        "  inputs={k:v.to(device) for k,v in batch.items()\n",
        "          if k in tokenizer.model_input_names}\n",
        "  #Extract last hidden states\n",
        "  with torch.no_grad():\n",
        "    last_hidden_state=model(**inputs).last_hidden_state\n",
        "    #Return vector for [CLS] token\n",
        "  return {\"hidden_state\":last_hidden_state[:,0].cpu().numpy()}"
      ],
      "metadata": {
        "id": "dxyd3fE9Xz0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions_encoded.set_format(\"torch\",columns=[\"input_ids\",\"attention_mask\",\"label\"])"
      ],
      "metadata": {
        "id": "jOxADIAAY80D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions_hidden=emotions_encoded.map(extract_hidden_states,batched=True)"
      ],
      "metadata": {
        "id": "ITLX4mzCb_Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions_hidden[\"train\"].column_names"
      ],
      "metadata": {
        "id": "pbF1b4TJcOxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_train=np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
        "X_valid=np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
        "y_train=np.array(emotions_hidden[\"train\"][\"label\"])\n",
        "y_valid=np.array(emotions_hidden[\"validation\"][\"label\"])\n",
        "X_train.shape,X_valid.shape"
      ],
      "metadata": {
        "id": "gi-20lqapUwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn #install umap-learn\n",
        "from umap import UMAP #import UMAP from the correct package\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#Scale features to [0,1] range\n",
        "X_scaled=MinMaxScaler().fit_transform(X_train)\n",
        "#Initialize and fit UMAP\n",
        "mapper=UMAP(n_components=2,metric=\"cosine\").fit(X_scaled)\n",
        "#Create a DataFrame of 2D embeddings\n",
        "df_emb=pd.DataFrame(mapper.embedding_,columns=[\"X\",\"Y\"])\n",
        "df_emb[\"label\"]=y_train\n",
        "df_emb.head()"
      ],
      "metadata": {
        "id": "o0L-YVDFqSxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming emotions_hidden is a datasets.DatasetDict\n",
        "fig, axes = plt.subplots(2, 3, figsize=(7, 5))\n",
        "axes = axes.flatten()\n",
        "cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
        "\n",
        "# Access labels from emotions_hidden\n",
        "labels =emotions[\"train\"].features[\"label\"].names\n",
        "\n",
        "for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
        "    df_emb_sub = df_emb.query(f\"label == {i}\")\n",
        "    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap, gridsize=20, linewidths=(0,))\n",
        "    axes[i].set_title(label)\n",
        "    axes[i].set_xticks([]), axes[i].set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rnYYFXj3rmcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_train=np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
        "X_valid=np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
        "y_train=np.array(emotions_hidden[\"train\"][\"label\"])\n",
        "y_valid=np.array(emotions_hidden[\"validation\"][\"label\"])\n",
        "X_train.shape,X_valid.shape\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#we increase 'max_iter' to guarantee convergence\n",
        "lr_clf=LogisticRegression(max_iter=3000)\n",
        "lr_clf.fit(X_train,y_train)\n",
        "lr_clf.score(X_valid,y_valid)"
      ],
      "metadata": {
        "id": "_GPw6m4ctnGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy_clf.fit(X_train, y_train)\n",
        "dummy_clf.score(X_valid, y_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "0CKZ2zXU7kLr",
        "outputId": "9a6ffa82-ccd6-4be7-d1e8-b35d309beb7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1ff20c54dbb0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdummy_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"most_frequent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdummy_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdummy_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "def plot_confusion_matrix(y_preds, y_true, labels):\n",
        " cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
        " fig, ax = plt.subplots(figsize=(6, 6))\n",
        " disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        " disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
        " plt.title(\"Normalized confusion matrix\")\n",
        " plt.show()\n",
        "y_preds = lr_clf.predict(X_valid)\n",
        "plot_confusion_matrix(y_preds, y_valid, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "UXwthfYF-pWZ",
        "outputId": "63d75d7b-d773-4bd6-d857-06c796d87ad4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lr_clf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f16b89a0c1a4>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m  \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Normalized confusion matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m  \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lr_clf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uIdjkWyr-sUa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}